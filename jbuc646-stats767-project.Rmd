---
title: "project"
author: "Joshua Buchanan"
date: "2025-03-19"
output: html_document
---

Loading in the dataset
```{r}


## Read dataset
rice <- read.csv("Rice_MSC_Dataset.csv")
#summary(rice)

## large outlier on kurtosis
## some variables have small number of NAs. 

#summary(scale(rice[,unlist(lapply(rice,is.numeric))]))
# Data from: https://www.muratkoklu.com/datasets/
# Read more about it in the articles linked. 
```

Removing the rows with NA values.
```{r}
table(rice$CLASS)
## Maybe let's try PCA on the colour variables to determine which ones are important to increase the interpretability of our results later. 

## Let's clean up missing or infinite values in the data to begin with
rice[which(is.na(rice)),]

sum(is.na(rice))

## Where are the NA values
num.na <- apply(rice, 2, FUN = function(x) { sum(is.na(x))})
num.na[num.na > 0]

## They are so minimal, let us remove the entire rows.

## Remove rows with NA
rice.filt <- na.omit(rice)
table(rice.filt$CLASS) ## It was Ipsala that was affected most.
```

Let's first use PCA with all variable in a effort to detect any large outliers in the dataset - there are no clear outliers here that we need to investigate.
```{r}
### PCA Plot - should any data points be removed
png(filename="PCAall.png")
all.vars.comp <- prcomp(rice.filt[,1:106], center=T, scale=T)
plot(all.vars.comp$x[,1],all.vars.comp$x[,2],col=as.numeric(as.factor(rice.filt$CLASS)), pch=19, cex=0.2, xlab="PC1 Score", ylab="PC2 Score")
legend(19,-6, legend=unique(rice.filt$CLASS), pch=19, cex=0.8, col=unique(as.numeric(as.factor(rice.filt$CLASS))))
## Add legend - comment on great seperation of groups which indicates that we should be able to predict classes well. 
## maybe also comment on some of the overlapping points e.g. the red ones which might be hard to predict. 
## Looking at it, the green group is most separated so should be the easiest one to predict on. 
```



Perform PCA on the colour variables for dimensionality reduction.
```{r}

## Create training and test indices - 70% training set will be used here. 
set.seed(23)
train.index <- sample(1:nrow(rice.filt), size=nrow(rice.filt) * 0.7)


## Perform PCA on the colour variables
names(rice.filt)

## Selecting only the 90 colour features
rice.colours <- rice.filt[train.index,17:106]

## Perform PCA
rice.comp <- prcomp(rice.colours, center=T, scale=T)
screeplot(rice.comp)

rice.comp$sdev ## stops dropping much after the sixth one. Could make argument for six PCs - maybe 3?

## Decide number of components with screeplot
plot(rice.comp$sd^2, xlab='component', ylab='variance', main='Rice screeplot')
abline(h=1)

## Cumulative sum of the variance explained by each PC. 
cumsum((rice.comp$sd^2)/sum(rice.comp$sd^2)) # The first six PCs account for 85.8% of the variability in the scaled data. 

```

Create the pairs plot of the first six PC scores against each other and colour points by type.

We see great separation between the types which indicates that predictability may be good. 

```{r}
## Pairs plot
png("pcpairs.png")
par(mar = c(0, 0, 0, 0))
pairs(rice.comp$x[,1:6], col=as.numeric(as.factor(rice.filt$CLASS)), pch=19, cex=0.2)
#plot.new()
# legend("topleft", 
#        legend = as.character(unique(as.factor(rice.filt$CLASS))), 
#        col = as.numeric(as.factor(unique(rice.filt$CLASS))), 
#        pch = 19, 
#        title = "Rice Types",cex=0.6)
```
How correlated are our variables?
```{r}
library(ggplot2)
library(reshape2)

corr <- cor(rice.reduced[,1:22])

# correlation heatmap
png("corplot.png")
diag(corr) <- NA
corr[lower.tri(corr)] <- NA
melted_corr <- melt(corr)
names(melted_corr) = c('Variable 1', 'Variable 2', 'correlation')
ggplot(melted_corr, aes(x=`Variable 1`, y=`Variable 2`, fill=correlation)) +
  geom_tile() +
  scale_fill_gradient2(low="#d73027", high="#4575b4") +
  guides(x = guide_axis(angle=90))

```


Perform LDA

```{r}
## The thought process is to reduce the dimension of colour variables into less PC variables so interpretability is high. 
## We will try classification with and without the reduced dimension.

## Checking equality of covariance using Box M's test:
library(biotools)
boxM_result <- boxM(rice.filt[,-107], rice.filt[,107])
boxM_result

## Let us add the PC variables to a new dataset with the other variables.
rice.reduced <- cbind(rice.filt[,1:16],
                      predict(rice.comp, rice.filt[,17:106])[,1:6],
                      CLASS=rice.filt[,107])

## Try LDA - remember to check assumptions
library(MASS)

## Create training and test indices - 70% training set will be used here. 
set.seed(23)
train.index <- sample(1:nrow(rice.reduced), size=nrow(rice.reduced) * 0.7)

rice.reduced.lda <- lda(rice.reduced[train.index,1:22], rice.reduced[train.index,23])
## predict
rice.reduced.lda.predict <- predict(rice.reduced.lda, rice.reduced[-train.index,-23])

```

Check assumptions. Check normality of LDA scores. 

```{r, fig.height=9, fig.width=4}
## Normality plot
png("norm.png")
rt <- unique(rice.filt$CLASS)
par(mar = rep(2,4))
par(mfrow=c(5,2))
for (i in 1:5) {
  hist(rice.lda.predict$x[rice.filt$CLASS == rt[i]], main=rt[i])
  qqnorm(rice.lda.predict$x[rice.filt$CLASS == rt[i]], main=paste("Normal Q-Q Plot for", rt[i]))
  qqline(rice.lda.predict$x[rice.filt$CLASS == rt[i]])
}

```


Do cross validation to determine number of functions to use. 
```{r}
#### Cross-validation to determine number of functions to use
## Create matrix
cv.predict <- matrix(NA, nrow = nrow(rice.reduced), ncol = 4)
colnames(cv.predict) <- c("One dimension", "Two dimensions",
                          "Three dimensions", "Four dimensions")

## Create 10 folds
set.seed(767) ## so it is the same each time we run
folds <- sample(rep(1:10, length.out = nrow(rice.reduced)))

## 10-fold cross-validation loop
for (k in 1:10) {
  ## Training and test data for the k-th fold
  train_indices <- which(folds != k)
  test_indices <- which(folds == k)
  
  ## Fit LDA model for training data
  lda.temp <- lda(rice.reduced[train_indices,-23], rice.reduced[train_indices,23])
  
  ## Predict for test data
  for (i in 4:1) {
    cv.predict[test_indices, i] <- predict(lda.temp,
                                           rice.reduced[test_indices,-23], dimen = i)$class
  }
}

# Print predictions for each number of LDA dimensions
colSums(cv.predict==as.numeric(as.factor(rice.reduced$CLASS)))

## Correct proportion classified
colSums(cv.predict==as.numeric(as.factor(rice.reduced$CLASS)))/nrow(rice.reduced)

## Adding fourth dimension is still a 0.27% increase on the third. I would suggest leaving all four for maximum accuracy. 
```


```{r}
## Function for evaluating the prediction quality
measure.classification <- function(cm){
  avg.accuracy <- 0
  err.rate <- 0
  ## Calculate average accuracy and error rate
  for (i in 1:ncol(cm)){
    avg.accuracy <- avg.accuracy + (cm[i,i] + sum(cm[-i,-i])) / sum(cm)
    err.rate <- err.rate + (sum(cm[i,-i]) + sum(cm[-i,i])) / sum(cm)
  }
  avg.accuracy <- avg.accuracy / i
  err.rate <- err.rate / i
  precision <- mean(diag(cm) / rowSums(cm))
  recall <- mean(diag(cm) / colSums(cm))
  F1.score <- 2 * precision * recall/(precision + recall)
  total.accuracy <- sum(diag(cm)) / sum(cm)
  return(list(avg.accuracy=round(avg.accuracy,4),
              err.rate=round(err.rate,4),
              precision=round(precision,4),
              recall=round(recall,4),
              F1.score=round(F1.score,4),
              tot.accuracy=round(total.accuracy,4)))
}
```

```{r}
## LDA reduced Confusion matrix
lda.cm <- table(rice.reduced.lda.predict$class, rice.reduced$CLASS[-train.index], dnn=c('Pred','Actual'))
lda.cm
measure.classification(lda.cm)
```

The prediction accuracy is even better here. 
```{r}
## do it also for the non-reduced dataset
rice.lda <- lda(rice.filt[train.index,1:106], rice.filt[train.index,107])
rice.lda.predict <- predict(rice.lda, rice.filt[-train.index,-107])
lda.cm.2 <- table(rice.lda.predict$class, rice.filt$CLASS[-train.index], dnn=c('Pred','Actual'))
lda.cm.2
measure.classification(lda.cm.2)
```


```{r}
## Interpret loadings as well. 
## Obviously when looking at these don't just look at the TEST split - look at total dataset. 
round(cor(rice.reduced[-train.index,1:22], rice.reduced.lda.predict$x),2)
```

We are no longer going for interpretability but are going for the maximum accuracy achieveable with this dataset.

```{r}
## Trying QDA
rice.qda <- qda(rice.filt[train.index,1:106], rice.filt[train.index,107])
rice.qda.predict <- predict(rice.qda, rice.filt[-train.index,-107])
qda.cm.2 <- table(rice.qda.predict$class, rice.filt$CLASS[-train.index], dnn=c('Pred','Actual'))
qda.cm.2
measure.classification(qda.cm.2)

## This actually reduced the accuracy compared to LDA.

```

Okay, let's try other methods in order to improve the accuracy. 
What should be next? Also when will I do this bloody write up? 


```{r}
## Cannot really show the correlations due to the sheer number of variables. 

## Could to 10-fold cross-validation to reduce the number of linear discriminants. 

## I think I need to explain in the intro in a way that we are going for the maximal prediction accuracy for what this may be used for
## This means we do not want to lose information - however, we will still try to reduce some dimensions to begin with and compared to see what we can get away with. 

## Try random forest
## Try gradient boosted trees for maximum predictive power - XGBoost
## Maybe SVM (RBF)? 
## How to use neural net to do this. 

## LDA, QDA?, Random Forest, SVM, XGBoost, Neural Net. 

```

Try random forest model

```{r}
## Random Forest Model
library(randomForest)
rf.rice <- randomForest(x=rice.filt[train.index,-107], y=as.factor(rice.filt[train.index,107]), ntree=500, mtry=3, importance=T)
```
Let's see how accurate it was
```{r}
rice.rf.predict <- predict(rf.rice, rice.filt[-train.index,-107])
rf.cm <- table(rice.rf.predict, as.factor(rice.filt$CLASS[-train.index]), dnn=c('Pred','Actual'))
rf.cm
measure.classification(rf.cm)
```
This model is having much less issue determining between Basmati and Jasmine compared to the full lda model. 


```{r}
## Save RData file since the random forest took so long to run.
save.image(file="rf-trained.RData")
```


Try SVM
```{r}
#XGBoost Model
library(xgboost)
xg.rice <- xgboost(data=as.matrix(rice.filt[train.index,-107]), 
                   label=as.numeric(as.factor(rice.filt[train.index,107]))-1,
                   objective="multi:softprob",
                   num_class=5,
                   nrounds=100,
                   eta=0.1,
                   max_depth=4, verbose=0)
```

Predict
```{r}
xg.predict <- matrix(predict(xg.rice, as.matrix(rice.filt[-train.index,-107])), nrow=5)
xg.pred.class <- max.col(t(xg.predict))

xg.cm <- table(xg.pred.class, rice.filt$CLASS[-train.index], dnn=c("Pred","Actual"))
```
Accuracy

```{r}
measure.classification(xg.cm)
```

Random forest model was the absolute best. 

Would need to put all of this into a table. Maybe begin writing the results up within LaTeX.


